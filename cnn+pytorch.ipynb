{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import Vectors\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam=pd.read_csv(r\"spam.csv\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text category\n",
       "0  Go until jurong point, crazy.. Available only ...      ham\n",
       "1                      Ok lar... Joking wif u oni...      ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...     spam\n",
       "3  U dun say so early hor... U c already then say...      ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...      ham"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing duplicates:  (5169, 2)\n",
      "Shape after removing null values:  (5169, 2)\n"
     ]
    }
   ],
   "source": [
    "spam = spam.drop_duplicates()\n",
    "print('Shape after removing duplicates: ', spam.shape)\n",
    "spam = spam.dropna()\n",
    "print('Shape after removing null values: ', spam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "spacy_en=spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words=spacy.lang.en.STOP_WORDS\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# First checking if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['text'] = spam['text'].str.lower()\n",
    "\n",
    "# removing punctuation marks\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "spam[\"text\"] = spam[\"text\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "\n",
    "# Removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOP_WORDS])\n",
    "spam[\"text\"] = spam[\"text\"].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "        \n",
    "# Lemmatizing the text data\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "spam[\"text\"] = spam[\"text\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# to clean data\n",
    "def normalise_text (text):\n",
    "    text = text.str.lower() # lowercase\n",
    "    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
    "    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
    "    text = text.str.replace(r\"@\",\"\")\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n",
    "    text = text.str.replace(\"\\s{2,}\", \" \")\n",
    "    return text\n",
    "\n",
    "spam[\"text\"] = normalise_text(spam[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spam = spam.drop_duplicates()\n",
    "spam = spam[spam['text'] != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(spam)\n",
    "train_df.to_csv('train_spam1.csv', index=False)\n",
    "valid_df.to_csv('test_spam1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT =data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = Field(sequential=False)\n",
    "\n",
    "#clean the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datafield = [('text', TEXT),  ('label', LABEL)]\n",
    "train = TabularDataset(path ='train.csv',  \n",
    "                             format='csv',\n",
    "                             skip_header=True,\n",
    "                             fields=train_datafield)\n",
    "\n",
    "\n",
    "#%%\n",
    "test_datafield = [('text', TEXT),  ('label',LABEL)]\n",
    "\n",
    "test = TabularDataset(path ='valid.csv', \n",
    "                       format='csv',\n",
    "                       skip_header=True,\n",
    "                       fields=test_datafield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'what', 'u', 'doing', 'today', '?'] ham\n",
      "['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...'] ham\n"
     ]
    }
   ],
   "source": [
    "print(train[0].text,  train[0].label)\n",
    "print(test[0].text,  test[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors= 'glove.6B.100d')\n",
    "LABEL.build_vocab(train)\n",
    "#%% load the pretrained embedding\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "\n",
    "train_iter = Iterator(\n",
    "        train, \n",
    "        batch_size=64,\n",
    "        device=torch.device('cuda'), \n",
    "        sort_within_batch=False,\n",
    "        repeat=False)\n",
    "\n",
    "test_iter = Iterator(test, batch_size=32, device=torch.device('cuda'), \n",
    "                     sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
    "        super(textCNN, self).__init__()\n",
    "        #load pretrained embedding in embedding layer.\n",
    "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
    "    \n",
    "        #Convolutional Layers with different window size kernels\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
    "        #Dropout layer\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        \n",
    "        #FC layer\n",
    "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "\n",
    "        con_x = [conv(emb_x) for conv in self.convs]\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        \n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        \n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "\n",
    "        fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "        \n",
    "\n",
    "#%% Training the Model\n",
    "def train(model, device, train_itr, optimizer, epoch, max_epoch):\n",
    "    model.train()\n",
    "    corrects, train_loss = 0.0,0\n",
    "    for batch in train_itr:\n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text,0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(text)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "    \n",
    "    size = len(train_itr.dataset)\n",
    "    train_loss /= size \n",
    "    accuracy = 100.0 * corrects/size\n",
    "  \n",
    "    return train_loss, accuracy\n",
    "    \n",
    "def valid(model, device, test_itr):\n",
    "    model.eval()\n",
    "    corrects, test_loss = 0.0,0\n",
    "    for batch in test_itr:\n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text,0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        \n",
    "        logit = model(text)\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "    \n",
    "    size = len(test_itr.dataset)\n",
    "    test_loss /= size \n",
    "    accuracy = 100.0 * corrects/size\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textCNN(\n",
      "  (embed): Embedding(7816, 100)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.6, inplace=False)\n",
      "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch: 1 \t Loss: 0.003690009249899632 \t Accuracy: 91.66666412353516%\n",
      "Valid Epoch: 1 \t Loss: 0.0034275024886461394 \t Accuracy: 95.93301391601562%\n",
      "model saves at 95.93301391601562 accuracy\n",
      "Train Epoch: 2 \t Loss: 0.0011505308713859472 \t Accuracy: 97.6153793334961%\n",
      "Valid Epoch: 2 \t Loss: 0.0022127470848235216 \t Accuracy: 97.96650695800781%\n",
      "model saves at 97.96650695800781 accuracy\n",
      "Train Epoch: 3 \t Loss: 0.0006175840132607099 \t Accuracy: 98.82051086425781%\n",
      "Valid Epoch: 3 \t Loss: 0.0018409502953758829 \t Accuracy: 98.20573425292969%\n",
      "model saves at 98.20573425292969 accuracy\n",
      "Train Epoch: 4 \t Loss: 0.00037075113421544816 \t Accuracy: 99.4358901977539%\n",
      "Valid Epoch: 4 \t Loss: 0.0016012926294655498 \t Accuracy: 98.38516235351562%\n",
      "model saves at 98.38516235351562 accuracy\n",
      "Train Epoch: 5 \t Loss: 0.00024046248744409053 \t Accuracy: 99.5128173828125%\n",
      "Valid Epoch: 5 \t Loss: 0.00196549581805507 \t Accuracy: 98.6842041015625%\n",
      "model saves at 98.6842041015625 accuracy\n",
      "Train Epoch: 6 \t Loss: 0.00011921036931781623 \t Accuracy: 99.87178802490234%\n",
      "Valid Epoch: 6 \t Loss: 0.0015172758413649734 \t Accuracy: 98.7440185546875%\n",
      "model saves at 98.7440185546875 accuracy\n",
      "Train Epoch: 7 \t Loss: 8.048870278379091e-05 \t Accuracy: 99.9230728149414%\n",
      "Valid Epoch: 7 \t Loss: 0.0015376195244948686 \t Accuracy: 98.6842041015625%\n",
      "Train Epoch: 8 \t Loss: 5.015606825532487e-05 \t Accuracy: 99.9230728149414%\n",
      "Valid Epoch: 8 \t Loss: 0.0017636744765025612 \t Accuracy: 98.80382537841797%\n",
      "model saves at 98.80382537841797 accuracy\n",
      "Train Epoch: 9 \t Loss: 3.473225192954907e-05 \t Accuracy: 99.94871520996094%\n",
      "Valid Epoch: 9 \t Loss: 0.0016938548730810938 \t Accuracy: 98.6842041015625%\n",
      "Train Epoch: 10 \t Loss: 2.3175276523360458e-05 \t Accuracy: 100.0%\n",
      "Valid Epoch: 10 \t Loss: 0.001743201691113244 \t Accuracy: 98.4449691772461%\n"
     ]
    }
   ],
   "source": [
    "model = textCNN(vocab, 100, 100, [3, 4 , 5] , 2).to('cuda')\n",
    "# print the model summery\n",
    "print(model)    \n",
    "    \n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "best_test_acc = -1\n",
    "\n",
    "# Use GPU if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 10+1):\n",
    "    #train loss\n",
    "    tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
    "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
    "    \n",
    "    ts_loss, ts_acc = valid(model, device, test_iter)\n",
    "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, ts_loss, ts_acc))\n",
    "    \n",
    "    if ts_acc > best_test_acc:\n",
    "        best_test_acc = ts_acc\n",
    "        #save paras(snapshot)\n",
    "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
    "        torch.save(model.state_dict(), \"textCNN_IMDB_best_valid\")\n",
    "        \n",
    "    train_loss.append(tr_loss)\n",
    "    train_acc.append(tr_acc)\n",
    "    test_loss.append(ts_loss)\n",
    "    test_acc.append(ts_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
