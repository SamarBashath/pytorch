{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import spacy\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import Vectors\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "spacy_en=spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words=spacy.lang.en.STOP_WORDS\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# First checking if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam=pd.read_csv(r\"spam.csv\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing duplicates:  (5169, 2)\n",
      "Shape after removing null values:  (5169, 2)\n"
     ]
    }
   ],
   "source": [
    "spam = spam.drop_duplicates()\n",
    "print('Shape after removing duplicates: ', spam.shape)\n",
    "spam = spam.dropna()\n",
    "print('Shape after removing null values: ', spam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['text'] = spam['text'].str.lower()\n",
    "\n",
    "# removing punctuation marks\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "spam[\"text\"] = spam[\"text\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "\n",
    "# Removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOP_WORDS])\n",
    "spam[\"text\"] = spam[\"text\"].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "        \n",
    "# Lemmatizing the text data\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "spam[\"text\"] = spam[\"text\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# to clean data\n",
    "def normalise_text (text):\n",
    "    text = text.str.lower() # lowercase\n",
    "    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
    "    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
    "    text = text.str.replace(r\"@\",\"\")\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n",
    "    text = text.str.replace(\"\\s{2,}\", \" \")\n",
    "    return text\n",
    "\n",
    "spam[\"text\"] = normalise_text(spam[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the preprossed dataset:  (5169, 2)\n",
      "Shape after removing duplicates from the preprossed dataset:  (5093, 2)\n",
      "Shape after removing datapoints with empty text:  (5092, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the preprossed dataset: ', spam.shape)\n",
    "\n",
    "spam = spam.drop_duplicates()\n",
    "print('Shape after removing duplicates from the preprossed dataset: ', spam.shape)\n",
    "spam = spam[spam['text'] != \"\"]\n",
    "print('Shape after removing datapoints with empty text: ', spam.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam['category'] = spam['category'].map({'spam': 1, 'ham': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text): # create a tokenizer function\n",
    "    \"\"\" A function for tokenization\"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = tokenizer, include_lengths = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, fields, is_test=False, **kwargs):\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.category if not is_test else None\n",
    "            text = row.text\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "        data_field = fields\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), data_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), data_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['pls', 'come', 'quick', 'ca', 'nt', 'bare'], 'category': 0}\n",
      "<class 'torchtext.data.example.Example'>\n"
     ]
    }
   ],
   "source": [
    "# creating torchtext fields\n",
    "fields = [('text',TEXT), ('category',LABEL)]\n",
    "\n",
    "# Making tabular datasets\n",
    "train_ds, val_ds = DataFrameDataset.splits(fields, train_df=train_df, val_df=valid_df)\n",
    "\n",
    "# Lets look at a random example\n",
    "print(vars(train_ds[15]))\n",
    "# Check the type \n",
    "print(type(train_ds[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "TEXT.build_vocab(train_ds, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = 'glove.6B.100d',\n",
    "                 unk_init = torch.Tensor.zero_)\n",
    "\n",
    "LABEL.build_vocab(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iter, valid_iter = data.BucketIterator.splits(\n",
    "    (train_ds, val_ds), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.2\n",
    "# padding\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)       \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        output = self.fc1(hidden)\n",
    "        output = self.dropout(self.fc2(output))\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_net(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7191, 100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)\n",
    "#  to initiaise padded to zeros\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device) \n",
    "\n",
    "# Defining Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch or iterators\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator):\n",
    "    \"\"\"\n",
    "    Training function\n",
    "    model: The LSTM_net model defined above\n",
    "    iterator: train and validation iterators with batch sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        loss = criterion(predictions, batch.category)\n",
    "        acc = binary_accuracy(predictions, batch.category)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    \"\"\"\n",
    "    Function for evaluation of the trained model\n",
    "    model: The LSTM_net model defined above\n",
    "    iterator: train and validation iterators with batch sizes\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            acc = binary_accuracy(predictions, batch.category)\n",
    "            \n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.413 | Train Acc: 89.65%\n",
      "\t Val. Acc: 87.42%\n",
      "\tTrain Loss: 0.230 | Train Acc: 94.66%\n",
      "\t Val. Acc: 93.33%\n",
      "\tTrain Loss: 0.198 | Train Acc: 95.69%\n",
      "\t Val. Acc: 90.71%\n",
      "\tTrain Loss: 0.158 | Train Acc: 97.11%\n",
      "\t Val. Acc: 93.83%\n",
      "\tTrain Loss: 0.148 | Train Acc: 97.45%\n",
      "\t Val. Acc: 96.49%\n",
      "\tTrain Loss: 0.146 | Train Acc: 97.59%\n",
      "\t Val. Acc: 95.42%\n",
      "\tTrain Loss: 0.141 | Train Acc: 97.57%\n",
      "\t Val. Acc: 96.55%\n",
      "\tTrain Loss: 0.138 | Train Acc: 97.49%\n",
      "\t Val. Acc: 96.64%\n",
      "\tTrain Loss: 0.147 | Train Acc: 97.73%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.133 | Train Acc: 97.63%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.136 | Train Acc: 97.50%\n",
      "\t Val. Acc: 96.57%\n",
      "\tTrain Loss: 0.136 | Train Acc: 97.63%\n",
      "\t Val. Acc: 96.57%\n",
      "\tTrain Loss: 0.139 | Train Acc: 97.32%\n",
      "\t Val. Acc: 96.57%\n",
      "\tTrain Loss: 0.142 | Train Acc: 97.45%\n",
      "\t Val. Acc: 96.57%\n",
      "\tTrain Loss: 0.142 | Train Acc: 97.68%\n",
      "\t Val. Acc: 96.57%\n",
      "\tTrain Loss: 0.136 | Train Acc: 97.71%\n",
      "\t Val. Acc: 96.57%\n",
      "\tTrain Loss: 0.144 | Train Acc: 97.73%\n",
      "\t Val. Acc: 96.57%\n",
      "\tTrain Loss: 0.134 | Train Acc: 97.68%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.139 | Train Acc: 97.16%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.140 | Train Acc: 97.59%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.139 | Train Acc: 97.73%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.135 | Train Acc: 97.60%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.146 | Train Acc: 97.58%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.144 | Train Acc: 97.71%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.136 | Train Acc: 97.60%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.135 | Train Acc: 97.79%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.140 | Train Acc: 97.27%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.138 | Train Acc: 97.47%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.139 | Train Acc: 97.63%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.143 | Train Acc: 97.44%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.140 | Train Acc: 97.81%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.135 | Train Acc: 97.68%\n",
      "\t Val. Acc: 96.50%\n",
      "\tTrain Loss: 0.136 | Train Acc: 97.47%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.143 | Train Acc: 97.50%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.138 | Train Acc: 97.70%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.141 | Train Acc: 97.58%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.144 | Train Acc: 97.37%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.141 | Train Acc: 97.23%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.137 | Train Acc: 97.27%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.132 | Train Acc: 97.58%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.136 | Train Acc: 97.47%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.138 | Train Acc: 97.58%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.142 | Train Acc: 97.33%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.139 | Train Acc: 97.42%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.136 | Train Acc: 97.60%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.141 | Train Acc: 97.71%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.133 | Train Acc: 97.50%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.132 | Train Acc: 97.47%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.131 | Train Acc: 97.77%\n",
      "\t Val. Acc: 96.58%\n",
      "\tTrain Loss: 0.142 | Train Acc: 97.60%\n",
      "\t Val. Acc: 96.58%\n",
      "time:29.955\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "loss=[]\n",
    "acc=[]\n",
    "val_acc=[]\n",
    "\n",
    "# Running the model and logging the results\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iter)\n",
    "    valid_acc = evaluate(model, valid_iter)\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    loss.append(train_loss)\n",
    "    acc.append(train_acc)\n",
    "    val_acc.append(valid_acc)\n",
    "    \n",
    "print(f'time:{time.time()-t:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
